{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impurity - Entropy & Gini"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three commonly used **impurity measures** used in binary decision trees: **Entropy, Gini index, and Classification Error.** \n",
    "\n",
    "- Entropy : a way to measure impurity :\n",
    "\n",
    "     $$Entropy= - \\sum_{j}^{} p_j \\log_2(p_j)$$\n",
    "   \n",
    " \n",
    "-  Gini index (a criterion to minimize the probability of misclassification):\n",
    "\n",
    "     $$Gini=1- \\sum_{j}^{} p_j^2$$\n",
    "\n",
    "\n",
    "- Classification Error:\n",
    "\n",
    "     $$ClassificationError=1−\\max(p_j)$$\n",
    "\n",
    "\n",
    "where $P_j$ is the probability of class $J$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **entropy is 0** if **all samples of a node belong to the same class**, and the **entropy** is **maximal** if we have a **uniform class distribution**. In other words, the entropy of a node (consist of **single class**) is zero because the **probability is 1** and **log (1) = 0**. **Entropy reaches maximum value when all classes in the node have equal probability**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![towardsdatascience blog](./img_source/decision_tree_entropy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.  Entropy of a group in which **all** examples belong to the **same class**: This is **not a good set for training**.\n",
    "\n",
    "$$Entropy=- 1*\\log_2(1) = -1 * 0 = 0 $$\n",
    "\n",
    "2.  Entropy of a group with **50%** in **either class**: This is **a good set for training**.\n",
    "\n",
    "$$Entropy=− (0.5 * \\log_2(0.5) + 0.5 * \\log_2(0.5)) = -1 * \\log_2(0.5) = -1 * -1 = 1 $$\n",
    "\n",
    "So, basically, the **entropy attempts to maximize the mutual information** (by constructing a **equal probability node**) in the decision tree.\n",
    "\n",
    "\n",
    "**Similar to entropy**, the **Gini index** is **maximal if the classes are perfectly mixed**, for example, in a binary class:\n",
    "\n",
    "$$ Gini Index = 1- \\sum_{j}^{} p_j^2 = 1-(p_0^2+p_1^2)  =  1- (0.5^2 + 0.5^2) = 0.5 $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain (IG)\n",
    "\n",
    "\n",
    "Using a decision algorithm, we start at the **tree root and split the data on the feature that results in the largest information gain (IG)**.\n",
    "\n",
    "We **repeat** this splitting procedure at **each child node down to the empty leaves**. This means that the samples at **each node all belong to the same class**. However, this can result in a **very deep tree with many nodes**, which can **easily lead to overfitting**. Thus, we typically want to **prune the tre**e by setting a **limit for the maximum depth** of the tree.\n",
    "\n",
    "Basically, using **IG**, we want to **determine** which **attribute** in a given set of training feature vectors is **most useful**. In other words, **IG** tells us **how important** a given **attribute** of the feature vectors is.\n",
    "\n",
    "We will use **IG** to decide the **ordering of attributes** in the nodes of a decision tree.\n",
    "The Information Gain (IG) can be defined as follows:\n",
    "\n",
    "$$IG(D_p) = I(D_p) - \\frac{N_{left}}{N_p}I(D_{left}) - \\frac{N_{right}}{N_p}I(D_{right})$$\n",
    "\n",
    "\n",
    "where **$I$ could be entropy, Gini index, or classification error**, **$D_p$, $D_{left} $, and $D_{right}$  are the dataset of the parent, left and right child node**.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain (IG) - Examples\n",
    "\n",
    "In this section, we'll get IG for a specific case as shown below:\n",
    "\n",
    "![Mermaid Github](./img_source/diag_example_IG.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- First, **IG with Classification Error $(IG_E)$**:\n",
    "\n",
    " $$Classification Error = 1-\\max p_j$$\n",
    " $$I_E(D_p) = 1 - \\frac{40}{80} = 1 - 0.5 = 0.5$$\n",
    " $$A:I_E(D_{left}) = 1 - \\frac{30}{40} = 1 - \\frac34 = 0.25 $$\n",
    " $$A:I_E(D_{right}) = 1 - \\frac{30}{40} = 1 - \\frac34 = 0.25$$\n",
    " $$IG(D_p) = I(D_p) - \\frac{N_{left}}{N_p}I(D_{left}) - \\frac{N_{right}}{N_p}I(D_{right}) $$\n",
    " $$A:IG_E = 0.5 - \\frac{40}{80} \\times 0.25 - \\frac{40}{80} \\times 0.25 = 0.5 - 0.125 - 0.125 = \\color{blue}{0.25}$$\n",
    " $$B:I_E(D_{left}) = 1 - \\frac{40}{60} = 1 - \\frac23 = \\frac13 $$\n",
    " $$B:I_E(D_{right}) = 1 - \\frac{20}{20} = 1 - 1 = 0$$\n",
    " $$ B:IG_E = 0.5 - \\frac{60}{80} \\times \\frac13 - \\frac{20}{80} \\times 0 = 0.5 - 0.25 - 0 = \\color{blue}{0.25} $$ \n",
    " \n",
    " \n",
    "        The information gains using the classification error as a splitting criterion are the same (0.25) in bo-th cases A and B.\n",
    "\n",
    "\n",
    "- Second, **IG with Gini index $(IG_G)$**:\n",
    "\n",
    " $$Gini Index = 1-\\sum_{j}{} p_j^2$$\n",
    " $$I_G(D_p) = 1 - \\left( \\left(\\frac{40}{80} \\right)^2 + \\left(\\frac{40}{80}\\right)^2 \\right) = 1 - (0.5^2+0.5^2) =  0.5$$\n",
    " $$A:I_G(D_{left}) = 1 - \\left( \\left(\\frac{30}{40} \\right)^2 + \\left(\\frac{10}{40}\\right)^2 \\right) =  1 - \\left( \\frac{9}{16} + \\frac{1}{16} \\right) = \\frac38 =  0.375$$\n",
    " $$A:I_G(D_{right}) = 1 - \\left( \\left(\\frac{30}{40} \\right)^2 + \\left(\\frac{10}{40}\\right)^2 \\right) =  1 - \\left( \\frac{9}{16} + \\frac{1}{16} \\right) = \\frac38 =  0.375$$\n",
    " \n",
    " $$A:I_G = 0.5 - \\frac{40}{80} \\times 0.375 - \\frac{40}{80} \\times 0.375 = \\color{blue}{0.125}$$\n",
    " $$B:I_G(D_{left}) = 1 - \\left( \\left(\\frac{20}{60} \\right)^2 + \\left(\\frac{40}{60}\\right)^2 \\right) =  1 - \\left( \\frac{9}{16} + \\frac{1}{16} \\right) = 1 - \\frac59 =  0.44$$\n",
    " $$BB:I_G(D_{right}) = 1 - \\left( \\left(\\frac{20}{20}\\right)^2 + \\left(\\frac{0}{20}\\right)^2 \\right) =  1 - (1+0) = 1 - 1 =  0 $$\n",
    " $$B:I_G = 0.5 - \\frac{60}{80} \\times 0.44 - 0 = 0.5 - 0.33 = \\color{blue}{0.17}$$\n",
    "     So, the Gini index favors the split B.\n",
    "\n",
    "\n",
    "\n",
    "- Third, **IG with Entropy $(IG_H)$**:\n",
    "$$ Entropy = -\\sum_jp_j\\log_2p_j $$\n",
    "$$ I_H(D_p) = - \\left( 0.5\\log_2(0.5) + 0.5\\log_2(0.5) \\right) = 1 $$\n",
    "$$ A:I_H(D_{left}) = - \\left( \\frac{30}{40}\\log_2 \\left(\\frac{30}{40} \\right) + \\frac{10}{40}\\log_2 \\left(\\frac{10}{40} \\right) \\right) = 0.81$$\n",
    "$$ A:I_H(D_{right}) = - \\left( \\frac{30}{40}\\log_2 \\left(\\frac{30}{40} \\right) + \\frac{10}{40}\\log_2 \\left(\\frac{10}{40} \\right) \\right) = 0.81$$\n",
    "$$ A:IG_H = 1 - \\frac{40}{80} \\times 0.81 - \\frac{40}{80} \\times 0.81 = \\color{blue}{0.19}$$\n",
    "$$ B:I_H(D_{left}) = - \\left( \\frac{20}{60}\\log_2 \\left(\\frac{20}{60} \\right) + \\frac{40}{60}\\log_2 \\left(\\frac{40}{60} \\right) \\right) = 0.92$$\n",
    "$$ B:I_H(D_{right}) = - \\left( \\frac{20}{20}\\log_2 \\left(\\frac{20}{20} \\right) + 0 \\right) = 0$$\n",
    "$$ B:IG_H = 1 - \\frac{60}{80} \\times 0.92 - \\frac{20}{80} \\times 0 = \\color{blue}{0.31}$$\n",
    "\n",
    "     So, the Entropy criterion favors the split B.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Illustration\n",
    "\n",
    "In this section, we'll plot the three impurity criteria we discussed in the previous section:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border: solid 1px black;\">\n",
    "    \n",
    "    \n",
    "``` python\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def gini(p):\n",
    "    return (p)*(1 - (p)) + (1 - p)*(1 - (1-p))\n",
    "\n",
    "def entropy(p):\n",
    "    return - p*np.log2(p) - (1 - p)*np.log2((1 - p))\n",
    "\n",
    "def classification_error(p):\n",
    "    return 1 - np.max([p, 1 - p])\n",
    "\n",
    "x = np.arange(0.0, 1.0, 0.01)\n",
    "ent = [entropy(p) if p != 0 else None for p in x]\n",
    "c_err = [classification_error(i) for i in x]\n",
    "\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "for j, lab, ls, c, in zip(\n",
    "      [ent,  gini(x), c_err],\n",
    "      ['Entropy', 'Gini Impurity', 'Misclassification Error'],\n",
    "      ['-', '-', '--', '-.'],\n",
    "      ['red', 'green', 'blue']):\n",
    "   line = ax.plot(x, j, label=lab, linestyle=ls, lw=1, color=c)\n",
    "\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(0.01, 0.85),\n",
    "         ncol=1, fancybox=True, shadow=False)\n",
    "\n",
    "ax.axhline(y=0.5, linewidth=1, color='k', linestyle='--')\n",
    "ax.axhline(y=1.0, linewidth=1, color='k', linestyle='--')\n",
    "\n",
    "plt.ylim([0, 1.1])\n",
    "plt.xlabel('p(j=1)')\n",
    "plt.ylabel('Impurity Index')\n",
    "plt.show()\n",
    "\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Github yf repo](./img_source/example_illustration.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
